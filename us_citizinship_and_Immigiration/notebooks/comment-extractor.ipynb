{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3a3fb1-007a-4b3f-8ebf-8f246587e872",
   "metadata": {},
   "source": [
    "### The following scripts used for extracting comments from youtube videos using Youtube API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc51a2e-d633-40fc-af51-4d7caa93c540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 2979\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID           Author                 Time  \\\n",
      "0         C1             @wsj  2025-01-29T15:05:06   \n",
      "1         C2      @inframatic  2025-01-29T15:54:57   \n",
      "2         C3  @tomfullery9710  2025-01-29T15:57:51   \n",
      "3         C4      @jasonj1376  2025-01-29T15:58:52   \n",
      "4         C5       @Sam-yf4kt  2025-01-29T16:05:36   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Trump sows fear among migrants with shock-and-...    169           52  \n",
      "1  Legal immigrants have nothing to fear, and all...     53            0  \n",
      "2  WSJ sows fear by misrepresenting the truth.  F...      1            0  \n",
      "3  Illegals should be in fear, as any wh break th...     27            0  \n",
      "4          Illegals - that was a convenient omission     22            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\1-29-25_WSJ_29_Jan_2025_xIkLgooN9LA.xlsx\n"
     ]
    }
   ],
   "source": [
    "# THE WALL STREET JOURNAL\n",
    "# 29 Jan 2025 (362 DAYS) in 27 Jan 2026\n",
    "# Video title: How Trump Is Planning to End Birthright Citizenship | WSJ\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"xIkLgooN9LA\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"1-29-25_WSJ_29_Jan_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e24b8e-a413-47f0-a36a-c38b00bd739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 4404\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID             Author                 Time  \\\n",
      "0         C1   @matiaslanzi6132  2026-01-25T09:27:28   \n",
      "1         C2      @iTrapForKash  2026-01-17T15:09:53   \n",
      "2         C3       @TeamSpinUSA  2026-01-05T07:22:46   \n",
      "3         C4  @ohlsenjulian3288  2026-01-03T06:48:43   \n",
      "4         C5      @tonyotto4930  2025-12-29T21:32:45   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Hahaha, this is not possible. Are you crazy? I...      0            0  \n",
      "1                           No more anchor âš“ï¸ babyâ€™s      0            0  \n",
      "2  If approved by the Supreme Court for ending bi...      0            0  \n",
      "3  14th amendment was for black slaves not refuge...      0            0  \n",
      "4  It's not about Donald Trump being able to chan...      7            1  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-6-25_CNN_6_Dec_2025_1H-sg4h43DY.xlsx\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "# 6 Dec 2025\n",
    "# Video title: Supreme Court agrees to decide if Trump may end birthright citizenship\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"1H-sg4h43DY\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-6-25_CNN_6_Dec_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff286f-1800-4b17-9305-6e18f50f4f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 894\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID            Author                 Time  \\\n",
      "0         C1       @CMyChannel  2026-01-26T14:55:24   \n",
      "1         C2        @WulfStrom  2026-01-17T23:39:03   \n",
      "2         C3          @EazyE69  2026-01-01T13:01:48   \n",
      "3         C4    @LucasPolk-j5n  2025-12-31T05:28:58   \n",
      "4         C5  @ramakrishna5480  2025-12-27T08:40:34   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                             No more anchor babies.      0            0  \n",
      "1  So that will open the crack for potentially be...      0            0  \n",
      "2           Laws are overturned all the time, enjoy.      0            0  \n",
      "3  Something must be done about this. You can't i...      0            0  \n",
      "4                                 1898 case really ðŸ˜‚      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\5-15-25_NYT_15_May_2025_gYg1Jaz6G3I.xlsx\n"
     ]
    }
   ],
   "source": [
    "# New York Times\n",
    "# 15 May 2025\n",
    "# Video title: Can Trump Really End Birthright Citizenship?\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"gYg1Jaz6G3I\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"5-15-25_NYT_15_May_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286c4a3-f526-469b-94dc-3912107eb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 1637\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID            Author                 Time  \\\n",
      "0         C1      @JudahEl-w2q  2026-01-27T02:26:11   \n",
      "1         C2  @da-profezor5178  2026-01-26T04:55:32   \n",
      "2         C3   @rubinyoung9355  2026-01-13T22:01:54   \n",
      "3         C4    @lesasveen9982  2025-12-31T07:02:39   \n",
      "4         C5  @TanyaPepper-p6x  2025-12-31T06:55:36   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Ok how is this valid? That will be only the in...      0            0  \n",
      "1               We need to start with his kids first      0            0  \n",
      "2  December 12, 2025\\n\\nDear President Donald J. ...      0            0  \n",
      "3  Native Americans were excluded from birthright...      3            0  \n",
      "4                                          Go Trump!      2            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-5-25_ABC_5_Dec_2025_XNpA19NMnHU.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ABC News\n",
    "# 5 Dec 2025\n",
    "# Video title: Supreme Court to take up President Trump's birthright citizenship case\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"XNpA19NMnHU\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-5-25_ABC_5_Dec_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ddca2-6bb0-4c06-8ede-8580537dd06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 2455\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                  Author                 Time  \\\n",
      "0         C1  @Alexandria.Washington  2026-01-05T00:26:19   \n",
      "1         C2        @NajahSalman-z1u  2025-05-18T16:03:02   \n",
      "2         C3     @LorenzoNambo-jt7yp  2025-05-16T15:16:32   \n",
      "3         C4            @southendbos  2025-05-09T14:28:10   \n",
      "4         C5      @conniejohnson9511  2025-05-09T02:59:37   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Autochthon Indigenous â€œBlackâ€ \\nAmerican-India...      0            0  \n",
      "1  This man is very bad or crazy he wants to ruin...      0            0  \n",
      "2   That will lower the migration to the USA by 100%      0            0  \n",
      "3  Except for Rubio. Rubio is a birthright citize...      0            0  \n",
      "4  Weâ€™re not the only country that has it.  He ab...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-9-24_NBC_9_Dec_2024_2LnMTjpuO1o.xlsx\n"
     ]
    }
   ],
   "source": [
    "# NBC News\n",
    "# 9 Dec 2024\n",
    "# Video title: Trump says heâ€™d end birthright citizenship through executive action if he can\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"2LnMTjpuO1o\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-9-24_NBC_9_Dec_2024_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111bb99-fa6b-4590-87c5-fd28f4c9096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 6655\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                    Author                 Time  \\\n",
      "0         C1           @joyceyaffe1832  2026-01-26T23:56:59   \n",
      "1         C2               @laurab9867  2026-01-26T23:06:52   \n",
      "2         C3           @alinalindt4676  2026-01-26T21:50:35   \n",
      "3         C4        @jorgemelendez9546  2026-01-26T03:37:21   \n",
      "4         C5  @lynneleeluckdowsing6654  2026-01-26T01:01:09   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Trump's wife was born in another country. Shou...      1            0  \n",
      "1  If you are an immigrant... \\n\\nPROTECT YOURSEL...      0            0  \n",
      "2  Seems like war criminals are welcomed as long ...      0            0  \n",
      "3  Heâ€™s the president to gobern thatâ€™s why was el...      1            0  \n",
      "4  All countries should inform the US that they a...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-19-25_DemoNow_19_Dec_2025_W8dXD8-z6-4.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Democracy Now!\n",
    "# 19 Dec 2025\n",
    "# Video title: Trump Moves to Denaturalize Citizens, End Birthright Citizenship, Halt Visa Lottery\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"W8dXD8-z6-4\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-19-25_DemoNow_19_Dec_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b573b0-2465-496a-b9f4-a64d2cce4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 1186\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID            Author                 Time  \\\n",
      "0         C1  @abbyonomorelies  2025-12-13T19:47:04   \n",
      "1         C2    @USsuperiority  2025-10-28T16:01:44   \n",
      "2         C3  @NWstockoperator  2025-11-11T04:56:57   \n",
      "3         C4  @paulaharvey1171  2025-11-13T13:14:33   \n",
      "4         C5  @NWstockoperator  2025-11-13T15:45:34   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  14th Amendment written to benefit freed slaves...      1            0  \n",
      "1  So what will happen to Barron Trump son of an ...      1            3  \n",
      "2  Baron and Don jrs father is a US citizen. Wher...      1            0  \n",
      "3  Legal immigrant. Nothing to do with illegal fo...      0            0  \n",
      "4                               â€‹@paulaharvey1171yes      1            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\9-29-25_CNN_29_Sept_2025_A8CY5W9Ek0I.xlsx\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "# 29 Sept 2025\n",
    "# Video title: Trump asks Supreme Court to end birthright citizenship\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"A8CY5W9Ek0I\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"9-29-25_CNN_29_Sept_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3fce0-27ce-4b98-ab1c-f3db1f4509db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 4058\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID            Author                 Time  \\\n",
      "0         C1  @greghammett5739  2026-01-19T00:30:06   \n",
      "1         C2        @TrueMohax  2026-01-25T09:59:06   \n",
      "2         C3  @greghammett5739  2026-01-25T14:12:05   \n",
      "3         C4        @TrueMohax  2026-01-25T18:43:41   \n",
      "4         C5  @greghammett5739  2026-01-25T20:08:52   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Stop the criminal actions of the Supreme Goons...      0            4  \n",
      "1                No one is changing the Constitution      0            0  \n",
      "2  â€‹@TrueMohaxAnyone else want to respond to this...      0            0  \n",
      "3  @greghammett5739\\nWhat is he doing with those ...      0            0  \n",
      "4  â€‹@TrueMohaxFirst and foremost anyone that tell...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-6-25_KMPH_6_Dec_2025_pZlO5yvWVf4.xlsx\n"
     ]
    }
   ],
   "source": [
    "# KMPH FOX26 NEWS\n",
    "# 6 Dec 2025\n",
    "# Video title: A case to end birthright citizenship\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"pZlO5yvWVf4\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-6-25_KMPH_6_Dec_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae0641-6c54-426b-a1ea-03a9409e55c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 638\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID           Author                 Time  \\\n",
      "0         C1  @marksteele3876  2026-01-02T08:29:32   \n",
      "1         C2       @sheepbay3  2025-12-24T13:57:48   \n",
      "2         C3    @beepuffs5871  2025-12-29T10:48:40   \n",
      "3         C4         @pandy7x  2025-12-08T03:28:00   \n",
      "4         C5     @johndye9463  2025-11-26T13:15:04   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Ironically unless youâ€™re a NATIVE, no-one in A...      0            0  \n",
      "1  I hope Trump wins This !!!!!!    We need to pr...      0            1  \n",
      "2  Come on how does it affect anybody in any way ...      0            0  \n",
      "3  Natives are probably like \" F'YEAH. \"  What's ...      0            0  \n",
      "4  Everyone knows that if it were anyone else the...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\7-25-25_CBS_25_Jul_2025_cecRYTlseIg.xlsx\n"
     ]
    }
   ],
   "source": [
    "# CBS NEWS\n",
    "# 25 Jul 2025\n",
    "# Video title: Trump's birthright citizenship order faces legal blow\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"cecRYTlseIg\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"7-25-25_CBS_25_Jul_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a7c66-6f9b-4192-8fdc-ba149d4a77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 848\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID          Author                 Time  \\\n",
      "0         C1  @blackbird-one  2025-08-28T19:53:49   \n",
      "1         C2     @koneos6580  2025-07-27T18:58:50   \n",
      "2         C3         @zz532a  2025-07-27T14:34:11   \n",
      "3         C4  @RoxannOkamura  2025-07-27T03:51:04   \n",
      "4         C5  @gregkotyk3445  2025-07-27T03:21:03   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  The framers of the 14th Amendment drew a clear...      1            0  \n",
      "1  Oh mine. If the law passes, 99% of all America...      0            0  \n",
      "2   Without birthright citizenship who is a citizen?      0            0  \n",
      "3  What is his problem that's been a bill since t...      0            0  \n",
      "4            Wake up America you are all immigrants.      0            1  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\7-25-25_FACEZNATION_25_Jul_2025_KVn3H-biBhc.xlsx\n"
     ]
    }
   ],
   "source": [
    "# FACE THE NATION\n",
    "# 25 Jul 2025\n",
    "# Video title: Trump's efforts to end birthright citizenship hit roadblock\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"KVn3H-biBhc\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"7-25-25_FACEZNATION_25_Jul_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038b9cc-afeb-4111-ac5d-ae224c69704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 405\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                  Author                 Time  \\\n",
      "0         C1  @carolinerothstein5258  2025-10-11T22:37:51   \n",
      "1         C2   @annabellesanchez8011  2025-08-06T17:33:06   \n",
      "2         C3        @danielogrady303  2025-08-05T00:53:50   \n",
      "3         C4        @skepticynic5150  2025-07-18T12:46:56   \n",
      "4         C5                @uap1234  2025-07-15T19:15:43   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  So now they want to protect the babies, then s...      0            0  \n",
      "1      A federal judge has no say so! So ridiculous!      1            0  \n",
      "2                             Yoyr not blocking shit      0            0  \n",
      "3  Fine. The babies can stay. The illegal parents...      0            0  \n",
      "4  All white americans could fit in one state. Wh...      1            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\7-10-25_CNN_10_Jul_2025_TpeeA3brCj4.xlsx\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "# 10 Jul 2025\n",
    "# Video title: Federal judge blocks Trump's birthright citizenship order\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"TpeeA3brCj4\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"7-10-25_CNN_10_Jul_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b0465-c11b-486f-8af1-04f2b7ad1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da69b7a-0b74-47ba-81ad-17664ff1e1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 3816\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                          Author                 Time  \\\n",
      "0         C1                   @ANDREW-eq8lf  2026-01-20T23:07:13   \n",
      "1         C2                 @LOTSAPOPPA4765  2026-01-20T18:39:30   \n",
      "2         C3  @joseoswaldomartinezrodrigu154  2026-01-17T09:32:28   \n",
      "3         C4       @Cookie_monster_deathStar  2026-01-12T22:40:06   \n",
      "4         C5                @lisawilliams345  2026-01-09T17:06:34   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                                      Thank you ðŸ˜Š â¤      0            0  \n",
      "1  What all you immigrants are missing or ignorin...      0            0  \n",
      "2         Te cruzÃ³ con una bestia y nazca una plaga.      0            0  \n",
      "3                       God bless Donald J Trump ðŸ‘ðŸ‡ºðŸ‡²      0            0  \n",
      "4  Seems like anything that was created to give r...      0            1  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\6-27-25_LiveNOW_27_Jun_2025_qgrhpN7K-Q4.xlsx\n"
     ]
    }
   ],
   "source": [
    "# LiveNOW from FOX\n",
    "# 27 Jun 2025\n",
    "# Video title: \"Meant for babies of slaves\" Trump on birthright citizenship ruling\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"qgrhpN7K-Q4\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"6-27-25_LiveNOW_27_Jun_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82902e8-bace-4c65-b4a0-7a3ddeb9701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 1360\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                           Author                 Time  \\\n",
      "0         C1           @Alexandria.Washington  2026-01-20T04:22:29   \n",
      "1         C2                      @joetube144  2025-12-21T15:49:11   \n",
      "2         C3  @squidgameisaliarisawnosqui5341  2025-12-16T13:47:34   \n",
      "3         C4                      @julesj5853  2025-12-13T01:12:30   \n",
      "4         C5                 @lauraalston8012  2025-11-27T23:07:39   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  American SLAVES were - Imported â€œAfricansâ€, wh...      0            0  \n",
      "1  Not true. Read the Constitution word for word....      0            0  \n",
      "2                                       Get them out      0            0  \n",
      "3  That is not what the 14th ammendment says thou...      1            0  \n",
      "4  Illegals have no right in our country!\\nNo one...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\1-23-25_MSNOW_23_Jan_2025_fA3KIogjDn4.xlsx\n"
     ]
    }
   ],
   "source": [
    "# MS NOW\n",
    "# 23 Jan 2025\n",
    "# Video title: Trump attempts to end birthright citizenship\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"fA3KIogjDn4\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"1-23-25_MSNOW_23_Jan_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcc7ad-a5e3-442c-8d19-7c0c0a2840ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 1339\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID             Author                 Time  \\\n",
      "0         C1           @pandy7x  2025-12-08T03:38:52   \n",
      "1         C2        @njmcatnaps  2025-12-06T21:15:56   \n",
      "2         C3      @Kathleen1212  2025-12-01T09:27:11   \n",
      "3         C4      @Kathleen1212  2025-12-01T09:25:56   \n",
      "4         C5  @susanehamlin8550  2025-11-30T15:55:15   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Natives are probably like \" F'YEAH. \"  What's ...      0            0  \n",
      "1              BLOCK THEM, THANK YOU PRESIDENT TRUMP      0            0  \n",
      "2                Exactly, OUR VOICE IS OUR VOTE!!!!!      0            0  \n",
      "3  Yes Congress is failing us.  I voted for Presi...      0            0  \n",
      "4  Donald J. Trump is also a birthright citizen, ...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\9-27-25_LiveNOW_27_Sept_2025_VPYrw7XreYQ.xlsx\n"
     ]
    }
   ],
   "source": [
    "# LiveNOW from FOX\n",
    "# 27 Sept 2025\n",
    "# Video title: BREAKING: Trump urges Supreme Court to allow birthright citizenship restrictions\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"VPYrw7XreYQ\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"9-27-25_LiveNOW_27_Sept_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ad413-6dca-46ba-801e-cb6265f018d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 6224\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                Author                 Time  \\\n",
      "0         C1      @matthewware8973  2025-07-10T20:42:40   \n",
      "1         C2          @Deleted8790  2025-05-10T07:39:04   \n",
      "2         C3  @egyptiansoldier6535  2025-04-25T10:54:48   \n",
      "3         C4      @matthewware8973  2025-07-10T20:43:11   \n",
      "4         C5  @egyptiansoldier6535  2025-07-10T22:30:44   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                                    Trump â¤ Epstein      0            0  \n",
      "1               See how his vows turn out to be lies      0            0  \n",
      "2  It is true tho. America is the only country th...      0            2  \n",
      "3  Most north and south American countries have b...      0            0  \n",
      "4  @matthewware8973right but you gotta agree tho ...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-11-24_Valuetainment_11_Dec_2024_7eRSz5GUA8A.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Valuetainment\n",
    "# 11 Dec 2024\n",
    "# Video title: Trump Vows to End Birthright Citizenship and Overhaul Costly Migrant Court System\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"7eRSz5GUA8A\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-11-24_Valuetainment_11_Dec_2024_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440b5f2-55dd-4298-b420-1510f60d42f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 1960\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID              Author                 Time  \\\n",
      "0         C1   @kennethwells2465  2026-01-27T16:06:19   \n",
      "1         C2  @judyfernandez3100  2026-01-27T16:05:01   \n",
      "2         C3            @mfsebcw  2026-01-27T16:02:56   \n",
      "3         C4  @pinkawearnezz3231  2026-01-27T16:02:28   \n",
      "4         C5       @YosefSchmidt  2026-01-27T16:02:25   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  What about driving while black? Blacks still a...      0            0  \n",
      "1  None of these attacks on U.S citizens is norma...      0            0  \n",
      "2  What a lying sack of schitt. Nobody with an ou...      0            0  \n",
      "3  Glad to see some of them actually have there o...      0            0  \n",
      "4  If you want to find the mother load of murdere...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\1-27-26_MSNOW_27_Jan_2026_ow2aidHU44o.xlsx\n"
     ]
    }
   ],
   "source": [
    "# MS NOW\n",
    "# 27 Jan 2026\n",
    "# Video title: 'Unconstitutional': MN Republican ends bid for governor citing Trump's immigration policy\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"ow2aidHU44o\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"1-27-26_MSNOW_27_Jan_2026_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2aabcc-c9ea-4c77-b49b-4f989cb131c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 861\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                  Author                 Time  \\\n",
      "0         C1      @kondeladunami7918  2026-01-18T23:22:47   \n",
      "1         C2           @anesevol5758  2026-01-17T22:37:25   \n",
      "2         C3  @epsteinfilesfunisland  2026-01-14T00:28:51   \n",
      "3         C4                 @bmax43  2026-01-12T03:21:16   \n",
      "4         C5      @maureenmccall4381  2026-01-11T23:58:09   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0              President is prejudice against Latino      0            0  \n",
      "1  \"Honest mistakes\". Why is it then that the sam...      0            0  \n",
      "2  wow, white people REALLY want to keep America ...      0            0  \n",
      "3  American citizens do not have to cowl down to ...      0            0  \n",
      "4                                Start with mellania      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-18-25_FirstPost_18_Dec_2025_VbH4d8gZs9I.xlsx\n"
     ]
    }
   ],
   "source": [
    "# First Post\n",
    "# 18 Dec 2025\n",
    "# Video title: Trump Immigration Crackdown to Hit Naturalised US Citizens In 2026 | Spotlight | N18G\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"VbH4d8gZs9I\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-18-25_FirstPost_18_Dec_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0570d8-b932-45e0-b3ea-e43ed573556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 1465\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID               Author                 Time  \\\n",
      "0         C1    @therealcoachross  2026-01-26T06:10:45   \n",
      "1         C2  @nicholecollins6126  2026-01-24T15:39:52   \n",
      "2         C3   @altheatrotman5606  2026-01-23T23:44:53   \n",
      "3         C4  @scarletfrances5004  2026-01-23T20:25:32   \n",
      "4         C5            @lolately  2026-01-23T18:50:00   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Thank you President Trump! The American People...      0            0  \n",
      "1  I wonder if this includes people like Peter Th...      0            0  \n",
      "2                       That's right President Trump      0            0  \n",
      "3  Please have the IRS take a look at their taxes...      0            0  \n",
      "4  First off, why do we pay for childcare when th...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\1-15-26_ForbesBreakingNews_15_Jan_2026_TVvl9CrU9XA.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Forbes Breaking News\n",
    "# 15 Jan 2026\n",
    "# Video title: Trump Calls To Revoke Citizenship Of Any Naturalized Immigrant Convicted Of Defrauding Citizens\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"TVvl9CrU9XA\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"1-15-26_ForbesBreakingNews_15_Jan_2026_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592b75e-9ff7-45d0-932f-06c4e2a77ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 880\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                Author                 Time  \\\n",
      "0         C1  @kristelynmiller3084  2026-01-25T21:43:21   \n",
      "1         C2       @PacificCircle1  2025-12-30T02:36:36   \n",
      "2         C3        @cynthia121266  2025-12-28T09:25:41   \n",
      "3         C4        @cynthia121266  2025-12-27T00:59:00   \n",
      "4         C5        @cynthia121266  2025-12-27T00:55:22   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  â€œUndermine domestic tranquilityâ€! So, what abo...      0            0  \n",
      "1     Going full racist and Vance is doing the same.      0            0  \n",
      "2  ðŸ‡ºðŸ‡²ðŸ™ in the state of  Nevada  apartments were s...      0            0  \n",
      "3                   ðŸ‡ºðŸ‡²ðŸ™ making the situations worse.      0            0  \n",
      "4  ðŸ‡ºðŸ‡²ðŸ™ many black or Africans  don't have  where ...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\11-29-25_CBSEVENING&CBS_29_Nov_2025_4AyMeUNpnPU.xlsx\n"
     ]
    }
   ],
   "source": [
    "# CBS Evening News & CBS News\n",
    "# 29 Nov 2025\n",
    "# Video title: Trump says he plans to end immigration from \"Third-World Countries\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"4AyMeUNpnPU\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"11-29-25_CBSEVENING&CBS_29_Nov_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46484d-6b40-436f-9048-1370396a8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 2284\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID            Author                 Time  \\\n",
      "0         C1      @Mypurplehat  2026-01-22T21:53:45   \n",
      "1         C2    @BushidoXBrown  2026-01-20T12:00:01   \n",
      "2         C3    @jessepaul2180  2026-01-19T18:35:10   \n",
      "3         C4  @carlamcneal1963  2026-01-19T18:29:57   \n",
      "4         C5  @carlamcneal1963  2026-01-19T18:23:26   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                     What about Melania and her son      0            0  \n",
      "1  nazi\\n\\nthis is literally conservative extremi...      0            0  \n",
      "2  nobody cares about your opinion illegal immigr...      0            0  \n",
      "3                                       Thank God. ðŸ˜‚      0            0  \n",
      "4  I think I love President Trump for getting unw...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\9-26-25_CNN_26_Sept_2025_8iaXBtTIq7w.xlsx\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "# 26 Sept 2025\n",
    "# Video title: Trump admin cracks down on legal immigration\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"8iaXBtTIq7w\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"9-26-25_CNN_26_Sept_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fb4aa-b1fa-4298-99a4-d213ca91cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 866\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                Author                 Time  \\\n",
      "0         C1         @AndreasW-p4r  2025-04-02T13:42:48   \n",
      "1         C2      @ayerimlopez5982  2025-04-02T02:53:34   \n",
      "2         C3           @G.I.Sity17  2025-03-25T21:33:05   \n",
      "3         C4       @bhammoudeh2794  2025-03-18T14:27:25   \n",
      "4         C5  @robertsontirado4478  2025-03-14T23:21:49   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  What about Melania trump and kids? I tell you ...      0            0  \n",
      "1  I remember back when we became US Citizens it ...      0            0  \n",
      "2                                               Come      0            0  \n",
      "3       Does that affect old I-10 old applications ?      0            0  \n",
      "4  A lot of non American women came to my face an...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\11-24-24_DarianImmigrationLaw_24_Nov_2024_JuyA3o2CTwI.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Darian Immigration Law\n",
    "# 24 Nov 2024\n",
    "# Video title: Trump's New Naturalization Policies Explained: What Immigrants Need to Know (so far)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"JuyA3o2CTwI\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"11-24-24_DarianImmigrationLaw_24_Nov_2024_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c2ec7-78d8-48ca-b63f-3b43361dfba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 204\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                 Author                 Time  \\\n",
      "0         C1             @AmanSS890  2025-12-19T13:20:31   \n",
      "1         C2         @infernus42254  2025-12-19T07:33:55   \n",
      "2         C3  @aaronthebarbaric7733  2025-12-05T14:27:13   \n",
      "3         C4         @MyThought2026  2025-12-03T10:45:08   \n",
      "4         C5          @chefstirston  2025-12-04T11:23:05   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Canada needs to start  doing the same to many ...      0            0  \n",
      "1                                             Peace.      0            0  \n",
      "2  Trump Administration giving the heathens The P...      0            0  \n",
      "3  The Judges to be blame for the death of the in...      3            1  \n",
      "4                                          its trump      1            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\11-28-25_NBC_28_Nov_2025_9fjF0lQihbc.xlsx\n"
     ]
    }
   ],
   "source": [
    "# NBC News\n",
    "# 28 Nov 2025\n",
    "# Video title: Trump administration orders reexamination of some green card holders\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"9fjF0lQihbc\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"11-28-25_NBC_28_Nov_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1b30f-196b-4667-90be-006e261c8ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 2875\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID               Author                 Time  \\\n",
      "0         C1        @agilbert8262  2026-01-13T07:32:00   \n",
      "1         C2         @Auntietoshi  2026-01-04T17:43:14   \n",
      "2         C3  @elenaroukhadze4518  2026-01-01T09:15:08   \n",
      "3         C4    @cllowboyking5152  2025-12-31T17:14:25   \n",
      "4         C5         @Neonfluxvid  2026-01-02T20:58:12   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  And he is only a 3rd generation U.S. citizen. ...      0            0  \n",
      "1       THIS BOILS DOWN TO A SUPREME DICTATORSHIP!ðŸ˜‚â¤      0            0  \n",
      "2  Bravo republicans! Simultaneously turning 46 m...      0            0  \n",
      "3  If trump is going after naturalized citizens, ...      1            1  \n",
      "4                                     Elon Musk too.      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\7-7-25_MSNOW_7_Jul_2025_MlXcO6_aA2Q.xlsx\n"
     ]
    }
   ],
   "source": [
    "# MS NOW\n",
    "# 7 Jul 2025\n",
    "# Video title: 'Everybody is now on the chopping block': Trump DOJ now targeting naturalized citizens\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"MlXcO6_aA2Q\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"7-7-25_MSNOW_7_Jul_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b311d-5ec1-4b01-a7b7-45c7bf0bacfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 391\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID               Author                 Time  \\\n",
      "0         C1  @colleenjackson4227  2026-01-15T03:56:13   \n",
      "1         C2  @DieselTechie-uo9we  2026-01-03T18:37:36   \n",
      "2         C3             @Snowlys  2025-12-26T02:26:31   \n",
      "3         C4          @AbLL-zf3ev  2025-12-02T15:12:25   \n",
      "4         C5       @anEvilRat2009  2025-11-28T21:13:14   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                                         Way to go!      0            0  \n",
      "1  Is that a joke?  We are on the opposite side o...      0            0  \n",
      "2  This is the least we could do. Should be doing...      0            0  \n",
      "3  Americans restrict visa to most countries but ...      0            0  \n",
      "4  Step 1: Ruin their country for sake of oil and...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\11-7-25_CBS_7_Nov_2025_NAE_zdHROP8.xlsx\n"
     ]
    }
   ],
   "source": [
    "# CBS NEWS\n",
    "# 7 Nov 2025\n",
    "# Video title: Trump administration may deny visas for immigrants with certain health conditions\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"NAE_zdHROP8\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"11-7-25_CBS_7_Nov_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cdcb5-b510-4e0e-a426-366dd3a4eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 864\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID             Author                 Time  \\\n",
      "0         C1         @DianaNuhn  2025-07-30T16:21:09   \n",
      "1         C2  @jonahjohnsen6519  2025-05-11T02:28:54   \n",
      "2         C3  @jonahjohnsen6519  2025-05-11T02:25:37   \n",
      "3         C4            @jf4872  2025-05-04T07:18:23   \n",
      "4         C5       @sunnymittra  2025-03-14T19:56:28   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Birthright citizenship is not for illegals fro...      1            0  \n",
      "1  What Trump appears to be doing.\\n\\nHypothetica...      0            0  \n",
      "2  The Quiet Revolution Against Birthright Citize...      0            0  \n",
      "3  The recent SCOTUS ruling made clear....Being b...      0            0  \n",
      "4  People deliberately give birth to children in ...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\1-21-25_NBC_21_Jan_2025_BhFXB9lYcIE.xlsx\n"
     ]
    }
   ],
   "source": [
    "# NBC NEWS\n",
    "# 21 Jan 2025\n",
    "# Video title: Trump birthright citizenship executive order expected to face legal challenges\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"BhFXB9lYcIE\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"1-21-25_NBC_21_Jan_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b1de2-2321-4f29-a8af-15bfe9f3cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 2904\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID          Author                 Time  \\\n",
      "0         C1  @DaveJorgenson  2025-03-27T23:10:47   \n",
      "1         C2     @bowler7922  2025-03-27T23:16:06   \n",
      "2         C3   @asthetic8131  2025-03-27T23:18:36   \n",
      "3         C4      @ä¼è¦‹çŒ¿æ¯”å¤-k8c  2025-03-27T23:48:31   \n",
      "4         C5   @THEDUDE32967  2025-03-28T00:26:20   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                   We have Schoolhouse Rock at home   7261           26  \n",
      "1                                                  ðŸ˜‚     21            0  \n",
      "2               Bill would be rolling in his grave ðŸ’”     51            0  \n",
      "3        I don't want the schoolhouse rock at home ðŸ˜­     17            0  \n",
      "4  I'm just a Bill, a possibly Unlawful Bill, Kic...     45            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\3-28-25_WashingtonPostUniverse_28_Mar_2025_g6OPnGaWawA.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Washington Post Universe\n",
    "# 28 Mar 2025\n",
    "# Video title: â€˜Proof of citizenshipâ€™ executive order explained\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"g6OPnGaWawA\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"3-28-25_WashingtonPostUniverse_28_Mar_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f0385-9e72-4dfb-84dc-fa26642d70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 1429\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                     Author                 Time  \\\n",
      "0         C1         @kaywasinjapan9497  2025-07-16T17:24:09   \n",
      "1         C2         @CREATIVEJUNKI8050  2025-05-24T05:01:24   \n",
      "2         C3  @trdrockwarrior_sport4432  2025-05-16T06:38:19   \n",
      "3         C4  @trdrockwarrior_sport4432  2025-05-16T06:37:05   \n",
      "4         C5  @trdrockwarrior_sport4432  2025-05-16T06:36:16   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  I wish they made it illegal to marry American ...      0            0  \n",
      "1  Brother every â€œriskâ€ you mentioned itâ€™s not up...      0            0  \n",
      "2  What is this they can optioned themselves they...      0            0  \n",
      "3  This 90 day bull crap needs to be changed to 3...      0            0  \n",
      "4  This 90 day bull crap needs to be changed to 3...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\11-30-24_DarianImmigrationLaw_30_Nov_2024_dHAwN4MognI.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Darian Immigration Law\n",
    "# 30 Nov 2024\n",
    "# Video title: Trump's New Marriage Green Card Policies Explained: What Spouses Need to Know (so far)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"dHAwN4MognI\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"11-30-24_DarianImmigrationLaw_30_Nov_2024_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3cbbfc-9838-456f-8515-722eabab442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 881\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID              Author                 Time  \\\n",
      "0         C1   @martinbernal3161  2025-09-28T02:16:20   \n",
      "1         C2     @kittycat-tn3rr  2025-09-24T02:39:07   \n",
      "2         C3     @kittycat-tn3rr  2025-09-24T02:38:54   \n",
      "3         C4  @FlaccoZLuvsMariah  2025-07-24T01:44:23   \n",
      "4         C5       @MdRashad-i4v  2025-07-19T05:28:52   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  I really hope that the US citizenship can be t...      0            0  \n",
      "1                                              11:41      1            0  \n",
      "2                                              11:40      0            0  \n",
      "3                                   !!A KHLOE!!ðŸ—¯ï¸ðŸ’âš ï¸      0            0  \n",
      "4  Md,nurul,Islam,md,asab,Ali,sonamukhi,kazipur,s...      1            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\6-27-25_ABC_27_Jun_2025_oOsC8W04Asw.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ABC NEWS\n",
    "# 27 Jun 2025\n",
    "# Video title: FULL SPECIAL REPORT: Trump speaks following SCOTUS ruling on birthright citizenship order\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"oOsC8W04Asw\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"6-27-25_ABC_27_Jun_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97d6c8-0cfd-42c0-adf5-d7018aac372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 600\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID            Author                 Time  \\\n",
      "0         C1       @sculpting2  2025-12-04T12:10:16   \n",
      "1         C2      @PyroShields  2025-12-02T19:11:31   \n",
      "2         C3       @sliferxxxx  2025-12-02T15:54:23   \n",
      "3         C4      @PyroShields  2025-12-02T19:12:59   \n",
      "4         C5  @royalalpha.1377  2025-12-02T02:11:18   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Absolutely against constitution and freedom.  ...      0            0  \n",
      "1  Does his new plan includes his wife or is this...      0            0  \n",
      "2  Hmm watch them have exceptions for politicians...      0            1  \n",
      "3                                   Or Trump's wife.      0            0  \n",
      "4                               Run it on them Trump      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\4-2-25_Top10Tales_2_Apr_2025_X5qjDDEBraw.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Top 10 Tales\n",
    "# 2 Apr 2025\n",
    "# Video title: The END of DUAL Citizenship? Trump's SHOCKING New Plan Revealed!\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"X5qjDDEBraw\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"4-2-25_Top10Tales_2_Apr_2025_{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6d78c-ba5c-4cb4-bd16-41372dd75997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments and all replies, this may take some time...\n",
      "Total comments fetched (including replies): 870\n",
      "\n",
      "First 5 comments:\n",
      "  Comment ID                Author                 Time  \\\n",
      "0         C1          @kristof_123  2025-12-31T13:22:50   \n",
      "1         C2            @JayProper  2025-12-23T14:49:16   \n",
      "2         C3  @lawandacontreras767  2025-12-14T15:34:59   \n",
      "3         C4      @WeirdScience-ET  2025-12-14T02:49:26   \n",
      "4         C5    @michaelthomas1916  2025-12-13T20:30:34   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  We heard Dar and Cathy Yarbrough were reported...      1            0  \n",
      "1  Now they get to experience what being black in...      0            0  \n",
      "2  It wonâ€™t stop until Congress gets off their A*...      0            0  \n",
      "3  We have not heard the story behind why these w...      0            0  \n",
      "4  Reasonable suspension is them not being the wh...      0            0  \n",
      "\n",
      "All comments saved to: YouTube_Comments\\12-8-25_MSNOW_8_Dec_2025 _3wknU22QVZI.xlsx\n"
     ]
    }
   ],
   "source": [
    "# MS NOW\n",
    "# 8 Dec 2025 \n",
    "# Video title: Growing reports of U.S. citizens being detained amid Trump's immigration crackdown\n",
    "\n",
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "api_key = \" \"  # type your YouTube API key here\n",
    "video_id = \"3wknU22QVZI\"  # YouTube video ID\n",
    "max_results_per_page = 100  # Max comments per API call (max 100)\n",
    "output_folder = \"YouTube_Comments\"  # Folder to save Excel files\n",
    "delay_between_requests = 0.1  # seconds\n",
    "max_retries = 3  # retries if a page fails\n",
    "# -----------------------------------------\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Remove characters that can break Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def fetch_comments(youtube, video_id):\n",
    "    \"\"\"Fetch top-level comments and all replies.\"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results_per_page,\n",
    "                    pageToken=next_page_token,\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error fetching page {next_page_token}, retry {retries}/{max_retries}: {e}\")\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {next_page_token}, skipping...\")\n",
    "            break\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            top_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_id = f\"C{comment_id_counter}\"\n",
    "            comments_data.append({\n",
    "                'Comment ID': comment_id,\n",
    "                'Author': top_snippet['authorDisplayName'],\n",
    "                'Time': datetime.strptime(top_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                'Comment Text': clean_text_for_excel(top_snippet['textDisplay']),\n",
    "                'Likes': top_snippet['likeCount'],\n",
    "                'Reply Count': item['snippet'].get('totalReplyCount', 0)\n",
    "            })\n",
    "            comment_id_counter += 1\n",
    "\n",
    "            # If top-level comment has replies, fetch all\n",
    "            total_replies = item['snippet'].get('totalReplyCount', 0)\n",
    "            if total_replies > 0:\n",
    "                parent_id = item['id']\n",
    "                reply_next_token = None\n",
    "                while True:\n",
    "                    retries_reply = 0\n",
    "                    while retries_reply < max_retries:\n",
    "                        try:\n",
    "                            reply_request = youtube.comments().list(\n",
    "                                part=\"snippet\",\n",
    "                                parentId=parent_id,\n",
    "                                maxResults=max_results_per_page,\n",
    "                                pageToken=reply_next_token,\n",
    "                                textFormat=\"plainText\"\n",
    "                            )\n",
    "                            reply_response = reply_request.execute()\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            retries_reply += 1\n",
    "                            print(f\"Error fetching replies for {parent_id}, retry {retries_reply}/{max_retries}: {e}\")\n",
    "                            time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch replies for {parent_id}, skipping...\")\n",
    "                        break\n",
    "\n",
    "                    for reply in reply_response.get('items', []):\n",
    "                        reply_snippet = reply['snippet']\n",
    "                        comment_id = f\"C{comment_id_counter}\"\n",
    "                        comments_data.append({\n",
    "                            'Comment ID': comment_id,\n",
    "                            'Author': reply_snippet['authorDisplayName'],\n",
    "                            'Time': datetime.strptime(reply_snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "                            'Comment Text': clean_text_for_excel(reply_snippet['textDisplay']),\n",
    "                            'Likes': reply_snippet['likeCount'],\n",
    "                            'Reply Count': 0  # replies to replies are not fetched further\n",
    "                        })\n",
    "                        comment_id_counter += 1\n",
    "\n",
    "                    reply_next_token = reply_response.get('nextPageToken')\n",
    "                    if not reply_next_token:\n",
    "                        break\n",
    "                    time.sleep(delay_between_requests)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    print(\"Scraping comments and all replies, this may take some time...\")\n",
    "    comments = fetch_comments(youtube, video_id)\n",
    "    print(f\"Total comments fetched (including replies): {len(comments)}\")\n",
    "\n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Display first 5 rows\n",
    "        print(\"\\nFirst 5 comments:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = os.path.join(output_folder, f\"12-8-25_MSNOW_8_Dec_2025 _{video_id}.xlsx\")\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"\\nAll comments saved to: {excel_filename}\")\n",
    "    else:\n",
    "        print(\"No comments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f287c07-d29c-48ae-8efb-a0a8ce39f7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_date</th>\n",
       "      <th>video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE WALL STREET JOURNAL</td>\n",
       "      <td>2025-01-29</td>\n",
       "      <td>How Trump Is Planning to End Birthright Citize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>2025-12-06</td>\n",
       "      <td>Supreme Court agrees to decide if Trump may en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New York Times</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>Can Trump Really End Birthright Citizenship?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC News</td>\n",
       "      <td>2025-12-05</td>\n",
       "      <td>Supreme Court to take up President Trump's bir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBC News</td>\n",
       "      <td>2024-12-09</td>\n",
       "      <td>Trump says heâ€™d end birthright citizenship thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Democracy Now!</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>Trump Moves to Denaturalize Citizens, End Birt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29 Sept 2025</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>Trump asks Supreme Court to end birthright cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KMPH FOX26 NEWS</td>\n",
       "      <td>2025-12-06</td>\n",
       "      <td>A case to end birthright citizenship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CBS NEWS</td>\n",
       "      <td>2025-07-25</td>\n",
       "      <td>Trump's birthright citizenship order faces leg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FACE THE NATION</td>\n",
       "      <td>2025-07-25</td>\n",
       "      <td>Trump's efforts to end birthright citizenship ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNN</td>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>Federal judge blocks Trump's birthright citize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LiveNOW from FOX</td>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>\"Meant for babies of slaves\" Trump on birthrig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>Trump attempts to end birthright citizenship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27 Sept 2025</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>BREAKING: Trump urges Supreme Court to allow b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Valuetainment</td>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>Trump Vows to End Birthright Citizenship and O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2026-01-27</td>\n",
       "      <td>'Unconstitutional': MN Republican ends bid for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>First Post</td>\n",
       "      <td>2025-12-18</td>\n",
       "      <td>Trump Immigration Crackdown to Hit Naturalised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Forbes Breaking News</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>Trump Calls To Revoke Citizenship Of Any Natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CBS Evening News &amp; CBS News</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>Trump says he plans to end immigration from \"T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26 Sept 2025</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>Trump admin cracks down on legal immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Darian Immigration Law</td>\n",
       "      <td>2024-11-24</td>\n",
       "      <td>Trump's New Naturalization Policies Explained:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NBC News</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>Trump administration orders reexamination of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2025-07-07</td>\n",
       "      <td>'Everybody is now on the chopping block': Trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CBS NEWS</td>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>Trump administration may deny visas for immigr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NBC NEWS</td>\n",
       "      <td>2025-01-21</td>\n",
       "      <td>Trump birthright citizenship executive order e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Washington Post Universe</td>\n",
       "      <td>2025-03-28</td>\n",
       "      <td>â€˜Proof of citizenshipâ€™ executive order explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Darian Immigration Law</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>Trump's New Marriage Green Card Policies Expla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ABC NEWS</td>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>FULL SPECIAL REPORT: Trump speaks following SC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Top 10 Tales</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>The END of DUAL Citizenship? Trump's SHOCKING ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2025-12-08</td>\n",
       "      <td>Growing reports of U.S. citizens being detaine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        channel video_date  \\\n",
       "0       THE WALL STREET JOURNAL 2025-01-29   \n",
       "1                           CNN 2025-12-06   \n",
       "2                New York Times 2025-05-15   \n",
       "3                      ABC News 2025-12-05   \n",
       "4                      NBC News 2024-12-09   \n",
       "5                Democracy Now! 2025-12-19   \n",
       "6                  29 Sept 2025 2025-12-19   \n",
       "7               KMPH FOX26 NEWS 2025-12-06   \n",
       "8                      CBS NEWS 2025-07-25   \n",
       "9               FACE THE NATION 2025-07-25   \n",
       "10                          CNN 2025-07-10   \n",
       "11             LiveNOW from FOX 2025-06-27   \n",
       "12                       MS NOW 2025-01-23   \n",
       "13                 27 Sept 2025 2025-01-23   \n",
       "14                Valuetainment 2024-12-11   \n",
       "15                       MS NOW 2026-01-27   \n",
       "16                   First Post 2025-12-18   \n",
       "17         Forbes Breaking News 2026-01-15   \n",
       "18  CBS Evening News & CBS News 2025-11-29   \n",
       "19                 26 Sept 2025 2025-11-29   \n",
       "20       Darian Immigration Law 2024-11-24   \n",
       "21                     NBC News 2025-11-28   \n",
       "22                       MS NOW 2025-07-07   \n",
       "23                     CBS NEWS 2025-11-07   \n",
       "24                     NBC NEWS 2025-01-21   \n",
       "25     Washington Post Universe 2025-03-28   \n",
       "26       Darian Immigration Law 2024-11-30   \n",
       "27                     ABC NEWS 2025-06-27   \n",
       "28                 Top 10 Tales 2025-04-02   \n",
       "29                       MS NOW 2025-12-08   \n",
       "\n",
       "                                          video_title  \n",
       "0   How Trump Is Planning to End Birthright Citize...  \n",
       "1   Supreme Court agrees to decide if Trump may en...  \n",
       "2        Can Trump Really End Birthright Citizenship?  \n",
       "3   Supreme Court to take up President Trump's bir...  \n",
       "4   Trump says heâ€™d end birthright citizenship thr...  \n",
       "5   Trump Moves to Denaturalize Citizens, End Birt...  \n",
       "6   Trump asks Supreme Court to end birthright cit...  \n",
       "7                A case to end birthright citizenship  \n",
       "8   Trump's birthright citizenship order faces leg...  \n",
       "9   Trump's efforts to end birthright citizenship ...  \n",
       "10  Federal judge blocks Trump's birthright citize...  \n",
       "11  \"Meant for babies of slaves\" Trump on birthrig...  \n",
       "12       Trump attempts to end birthright citizenship  \n",
       "13  BREAKING: Trump urges Supreme Court to allow b...  \n",
       "14  Trump Vows to End Birthright Citizenship and O...  \n",
       "15  'Unconstitutional': MN Republican ends bid for...  \n",
       "16  Trump Immigration Crackdown to Hit Naturalised...  \n",
       "17  Trump Calls To Revoke Citizenship Of Any Natur...  \n",
       "18  Trump says he plans to end immigration from \"T...  \n",
       "19       Trump admin cracks down on legal immigration  \n",
       "20  Trump's New Naturalization Policies Explained:...  \n",
       "21  Trump administration orders reexamination of s...  \n",
       "22  'Everybody is now on the chopping block': Trum...  \n",
       "23  Trump administration may deny visas for immigr...  \n",
       "24  Trump birthright citizenship executive order e...  \n",
       "25   â€˜Proof of citizenshipâ€™ executive order explained  \n",
       "26  Trump's New Marriage Green Card Policies Expla...  \n",
       "27  FULL SPECIAL REPORT: Trump speaks following SC...  \n",
       "28  The END of DUAL Citizenship? Trump's SHOCKING ...  \n",
       "29  Growing reports of U.S. citizens being detaine...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d5c34-8f1f-4bb1-89a7-9213777672c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_date</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE WALL STREET JOURNAL</td>\n",
       "      <td>2025-01-29</td>\n",
       "      <td>None</td>\n",
       "      <td>How Trump Is Planning to End Birthright Citize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>2025-12-06</td>\n",
       "      <td>None</td>\n",
       "      <td>Supreme Court agrees to decide if Trump may en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New York Times</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>None</td>\n",
       "      <td>Can Trump Really End Birthright Citizenship?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC News</td>\n",
       "      <td>2025-12-05</td>\n",
       "      <td>None</td>\n",
       "      <td>Supreme Court to take up President Trump's bir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBC News</td>\n",
       "      <td>2024-12-09</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump says heâ€™d end birthright citizenship thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Democracy Now!</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump Moves to Denaturalize Citizens, End Birt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29 Sept 2025</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump asks Supreme Court to end birthright cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KMPH FOX26 NEWS</td>\n",
       "      <td>2025-12-06</td>\n",
       "      <td>None</td>\n",
       "      <td>A case to end birthright citizenship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CBS NEWS</td>\n",
       "      <td>2025-07-25</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump's birthright citizenship order faces leg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FACE THE NATION</td>\n",
       "      <td>2025-07-25</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump's efforts to end birthright citizenship ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNN</td>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>None</td>\n",
       "      <td>Federal judge blocks Trump's birthright citize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LiveNOW from FOX</td>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>None</td>\n",
       "      <td>\"Meant for babies of slaves\" Trump on birthrig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump attempts to end birthright citizenship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27 Sept 2025</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>None</td>\n",
       "      <td>BREAKING: Trump urges Supreme Court to allow b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Valuetainment</td>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump Vows to End Birthright Citizenship and O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2026-01-27</td>\n",
       "      <td>None</td>\n",
       "      <td>'Unconstitutional': MN Republican ends bid for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>First Post</td>\n",
       "      <td>2025-12-18</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump Immigration Crackdown to Hit Naturalised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Forbes Breaking News</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump Calls To Revoke Citizenship Of Any Natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CBS Evening News &amp; CBS News</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump says he plans to end immigration from \"T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26 Sept 2025</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump admin cracks down on legal immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Darian Immigration Law</td>\n",
       "      <td>2024-11-24</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump's New Naturalization Policies Explained:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NBC News</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump administration orders reexamination of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2025-07-07</td>\n",
       "      <td>None</td>\n",
       "      <td>'Everybody is now on the chopping block': Trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CBS NEWS</td>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump administration may deny visas for immigr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NBC NEWS</td>\n",
       "      <td>2025-01-21</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump birthright citizenship executive order e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Washington Post Universe</td>\n",
       "      <td>2025-03-28</td>\n",
       "      <td>None</td>\n",
       "      <td>â€˜Proof of citizenshipâ€™ executive order explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Darian Immigration Law</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>None</td>\n",
       "      <td>Trump's New Marriage Green Card Policies Expla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ABC NEWS</td>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>None</td>\n",
       "      <td>FULL SPECIAL REPORT: Trump speaks following SC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Top 10 Tales</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>None</td>\n",
       "      <td>The END of DUAL Citizenship? Trump's SHOCKING ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MS NOW</td>\n",
       "      <td>2025-12-08</td>\n",
       "      <td>None</td>\n",
       "      <td>Growing reports of U.S. citizens being detaine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Date</td>\n",
       "      <td>2025-01-29</td>\n",
       "      <td>None</td>\n",
       "      <td>if \"Video title:\" in line:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Date</td>\n",
       "      <td>2025-01-29</td>\n",
       "      <td>None</td>\n",
       "      <td>current_title = line.replace(\"\", \"\").strip()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        channel video_date video_id  \\\n",
       "0       THE WALL STREET JOURNAL 2025-01-29     None   \n",
       "1                           CNN 2025-12-06     None   \n",
       "2                New York Times 2025-05-15     None   \n",
       "3                      ABC News 2025-12-05     None   \n",
       "4                      NBC News 2024-12-09     None   \n",
       "5                Democracy Now! 2025-12-19     None   \n",
       "6                  29 Sept 2025 2025-12-19     None   \n",
       "7               KMPH FOX26 NEWS 2025-12-06     None   \n",
       "8                      CBS NEWS 2025-07-25     None   \n",
       "9               FACE THE NATION 2025-07-25     None   \n",
       "10                          CNN 2025-07-10     None   \n",
       "11             LiveNOW from FOX 2025-06-27     None   \n",
       "12                       MS NOW 2025-01-23     None   \n",
       "13                 27 Sept 2025 2025-01-23     None   \n",
       "14                Valuetainment 2024-12-11     None   \n",
       "15                       MS NOW 2026-01-27     None   \n",
       "16                   First Post 2025-12-18     None   \n",
       "17         Forbes Breaking News 2026-01-15     None   \n",
       "18  CBS Evening News & CBS News 2025-11-29     None   \n",
       "19                 26 Sept 2025 2025-11-29     None   \n",
       "20       Darian Immigration Law 2024-11-24     None   \n",
       "21                     NBC News 2025-11-28     None   \n",
       "22                       MS NOW 2025-07-07     None   \n",
       "23                     CBS NEWS 2025-11-07     None   \n",
       "24                     NBC NEWS 2025-01-21     None   \n",
       "25     Washington Post Universe 2025-03-28     None   \n",
       "26       Darian Immigration Law 2024-11-30     None   \n",
       "27                     ABC NEWS 2025-06-27     None   \n",
       "28                 Top 10 Tales 2025-04-02     None   \n",
       "29                       MS NOW 2025-12-08     None   \n",
       "30                         Date 2025-01-29     None   \n",
       "31                         Date 2025-01-29     None   \n",
       "\n",
       "                                          video_title  \n",
       "0   How Trump Is Planning to End Birthright Citize...  \n",
       "1   Supreme Court agrees to decide if Trump may en...  \n",
       "2        Can Trump Really End Birthright Citizenship?  \n",
       "3   Supreme Court to take up President Trump's bir...  \n",
       "4   Trump says heâ€™d end birthright citizenship thr...  \n",
       "5   Trump Moves to Denaturalize Citizens, End Birt...  \n",
       "6   Trump asks Supreme Court to end birthright cit...  \n",
       "7                A case to end birthright citizenship  \n",
       "8   Trump's birthright citizenship order faces leg...  \n",
       "9   Trump's efforts to end birthright citizenship ...  \n",
       "10  Federal judge blocks Trump's birthright citize...  \n",
       "11  \"Meant for babies of slaves\" Trump on birthrig...  \n",
       "12       Trump attempts to end birthright citizenship  \n",
       "13  BREAKING: Trump urges Supreme Court to allow b...  \n",
       "14  Trump Vows to End Birthright Citizenship and O...  \n",
       "15  'Unconstitutional': MN Republican ends bid for...  \n",
       "16  Trump Immigration Crackdown to Hit Naturalised...  \n",
       "17  Trump Calls To Revoke Citizenship Of Any Natur...  \n",
       "18  Trump says he plans to end immigration from \"T...  \n",
       "19       Trump admin cracks down on legal immigration  \n",
       "20  Trump's New Naturalization Policies Explained:...  \n",
       "21  Trump administration orders reexamination of s...  \n",
       "22  'Everybody is now on the chopping block': Trum...  \n",
       "23  Trump administration may deny visas for immigr...  \n",
       "24  Trump birthright citizenship executive order e...  \n",
       "25   â€˜Proof of citizenshipâ€™ executive order explained  \n",
       "26  Trump's New Marriage Green Card Policies Expla...  \n",
       "27  FULL SPECIAL REPORT: Trump speaks following SC...  \n",
       "28  The END of DUAL Citizenship? Trump's SHOCKING ...  \n",
       "29  Growing reports of U.S. citizens being detaine...  \n",
       "30                         if \"Video title:\" in line:  \n",
       "31       current_title = line.replace(\"\", \"\").strip()  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ddb712-954e-4854-b4af-c549b2b4d8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
