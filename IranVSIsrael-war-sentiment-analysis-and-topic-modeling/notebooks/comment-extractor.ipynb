{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3a3fb1-007a-4b3f-8ebf-8f246587e872",
   "metadata": {},
   "source": [
    "### The following scripts used for extracting comments from youtube videos using Youtube API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b724f8fc-0e4c-49db-b3fe-18fd3888f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 2975 and saved to israel__Y10t-EhRbQ.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID               Author                 Time  \\\n",
      "0         C1    @Sugengadisaputra  2025-07-03T09:30:00   \n",
      "1         C2  @dr.suhailabbas4721  2025-06-30T17:59:14   \n",
      "2         C3  @AracelyEscobar-m5e  2025-06-26T16:46:24   \n",
      "3         C4          @Mark-m9z4q  2025-06-23T17:40:29   \n",
      "4         C5     @HaydenJones-t4i  2025-06-22T22:50:10   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0      Israhell without USA is nothing. A coward....      1            0  \n",
      "1  president Shara is traitor to muslim people\\na...      0            0  \n",
      "2  Well, Israel, we know you're doing the right t...      0            0  \n",
      "3  How long will it be before; yet another, Soros...      0            0  \n",
      "4  boo hoo hoo - If you left wing nutjobs don't l...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in CNN.\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# This is my Youtube API\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Here is the video id in Youtube\n",
    "video_id = \"_Y10t-EhRbQ\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039cdb3c-bb1d-4175-a3ad-df67ba74e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 1685 and saved to israel_BBC_SIbIj4ThXkE.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID                   Author                 Time  \\\n",
      "0         C1            @RailaMeckler  2025-07-05T23:32:52   \n",
      "1         C2  @ScholasticaPakan-dh2ei  2025-07-05T08:12:24   \n",
      "2         C3                 @Hagos·ãà·ã≤  2025-06-22T18:29:12   \n",
      "3         C4             @AnkrahPhiri  2025-06-22T13:53:16   \n",
      "4         C5                   @ilfyf  2025-06-22T09:41:47   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  By comparing replicas with genuine items, you ...      0            0  \n",
      "1  War is the Sign of the Second Coming of Jesus ...      0            0  \n",
      "2                 America helps for its girl friend.      0            0  \n",
      "3                                              üåèüòÆüåèüòÆ‚ù§      0            0  \n",
      "4  Films About the Horrors in Iran\\n\\n2008 ‚Äì Be L...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in BBC\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"SIbIj4ThXkE\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_BBC_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52b1a58-136a-49cd-898e-ad20b80035d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 1371 and saved to israel_ALJAZEERA_sz_GktoY7ig.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID                Author                 Time  \\\n",
      "0         C1  @BhekiVincentDlamini  2025-06-21T03:43:15   \n",
      "1         C2   @gilbertmercado2764  2025-06-19T17:34:56   \n",
      "2         C3   @gilbertmercado2764  2025-06-19T17:33:27   \n",
      "3         C4       @lloydrambeeran  2025-06-19T05:03:03   \n",
      "4         C5               @JvdNzr  2025-06-18T10:57:07   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Israel's self defense from what? Same lies net...      0            0  \n",
      "1                                       Shitirael!!!      0            0  \n",
      "2  The attack on Iran would have never happened i...      0            0  \n",
      "3  Iran dom  don't  no the art of wore  israel no it      0            0  \n",
      "4  ÿßÿ≥ÿ±ÿß€å€åŸÑ ÿÆŸàÿ® ŸÜŸÇÿ¥ ŸÖÿ∏ŸÑŸàŸÖ ÿ±ÿß ÿ®ÿßÿ≤€å ŸÖ€å⁄©ŸÜÿØ\\n⁄©ÿ¥Ÿàÿ±Ÿáÿß€å ÿß...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in AL JAZEERA\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"sz_GktoY7ig\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_ALJAZEERA_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281119db-e977-4b7d-a902-e40e5271d3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 2025 and saved to israel_ALJAZEERA_p82sQJ4L5sI.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID              Author                 Time  \\\n",
      "0         C1  @WhetuMotutere-d3k  2025-07-17T16:56:25   \n",
      "1         C2  @WhetuMotutere-d3k  2025-07-17T16:55:41   \n",
      "2         C3    @marcrichter9160  2025-07-16T19:14:10   \n",
      "3         C4    @marcrichter9160  2025-07-16T19:12:02   \n",
      "4         C5    @marcrichter9160  2025-07-16T18:55:09   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                America loosing grip on exsibistion      0            0  \n",
      "1  Its true iran has been very transparent isreal...      0            0  \n",
      "2  How on earth can you conduct diplomacy when yo...      0            0  \n",
      "3  The US and Israel and the Uk and E.U. are all ...      0            0  \n",
      "4  Israel and the US are the aggressors. Iran is ...      1            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in AL JAZEERA 2\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"p82sQJ4L5sI\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_ALJAZEERA_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe52597-ba29-414c-b292-27d4afc4d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 1426 and saved to israel_ALJAZEERA_dFv2uXZFmeo.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID                Author                 Time  \\\n",
      "0         C1     @aljazeeraenglish  2025-06-19T17:31:50   \n",
      "1         C2      @anthonyrios3178  2025-07-17T17:29:28   \n",
      "2         C3  @MysticTalkBobbyShue  2025-07-11T22:18:04   \n",
      "3         C4  @mangatsinghdeol2941  2025-07-09T04:33:50   \n",
      "4         C5             @Juan-k3h  2025-07-07T15:52:51   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  For more background on this story, watch our r...     85            8  \n",
      "1                       Israel needs to rethink this      0            0  \n",
      "2  Isreal is not our ally. Hamas is funded by Ham...      0            0  \n",
      "3  You have to be special kind of stupid to belie...      0            0  \n",
      "4                      Because Satanyahu, that's why      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in AL JAZEERA 3\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"dFv2uXZFmeo\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_ALJAZEERA_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c94c6e-adb4-4cca-acf1-8c0aa16983f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 4296 and saved to israel_SKYNEWS_LBviraT5L2A.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID             Author                 Time  \\\n",
      "0         C1  @maulidimikui3271  2025-07-16T20:01:39   \n",
      "1         C2    @orthorules1580  2025-07-16T12:22:05   \n",
      "2         C3      @degracia1098  2025-07-14T10:07:43   \n",
      "3         C4       @Mike24Black  2025-07-14T03:33:46   \n",
      "4         C5   @amirhossein7750  2025-07-13T20:29:25   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  But israel 80% of its arsenals are NATO eports...      0            0  \n",
      "1                                     Upper hand‚Ä¶lol      0            0  \n",
      "2                               Poor Iran no bunkerüòÇ      0            0  \n",
      "3  Interesting, informative, and with swag. Well ...      0            0  \n",
      "4  Meaning of the names of the missiles: Imad: me...      1            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in SKY NEWS\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"LBviraT5L2A\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_SKYNEWS_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38fbcf5b-b7fa-4376-a193-3930ff2d02ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 2240 and saved to israel_BBC_psRwnjuYumU.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID               Author                 Time  \\\n",
      "0         C1    @LauraBurnett-t3e  2025-07-01T22:18:31   \n",
      "1         C2           @pie_pooya  2025-06-21T17:48:57   \n",
      "2         C3  @mdfarukhossain5946  2025-06-20T17:19:46   \n",
      "3         C4         @ccbarker120  2025-06-20T16:45:00   \n",
      "4         C5      @ArbiAnsyah-y5c  2025-06-20T15:21:36   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                   You just earned a new subscriber      0            0  \n",
      "1  Iran broke world record with how fast it lost ...      0            0  \n",
      "2                                         Bangladesh      0            0  \n",
      "3                       BBC Spin masters propaganda.      0            0  \n",
      "4  Liar liar liar. Never trust shitty western med...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in BBC 2\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"psRwnjuYumU\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_BBC_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ffc563-477d-4774-9ffa-827c8eb4dbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 1900 and saved to israel_FOX_NfiVPAQapw8.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID                 Author                 Time  \\\n",
      "0         C1        @gandindian1234  2025-06-22T17:49:40   \n",
      "1         C2  @LimitlessDrive-b8b3x  2025-06-21T09:59:43   \n",
      "2         C3  @LimitlessDrive-b8b3x  2025-06-21T09:59:34   \n",
      "3         C4  @LimitlessDrive-b8b3x  2025-06-21T09:59:25   \n",
      "4         C5  @LimitlessDrive-b8b3x  2025-06-21T09:59:16   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  ◊ê◊†◊ô ◊ê◊¢◊©◊î ◊î◊õ◊ú, ◊ï◊ê◊ñ ◊ê◊ù ◊û◊ô◊©◊î◊ï ◊ô◊¢◊©◊î ◊û◊©◊î◊ï ◊ú◊ê ◊ë◊°◊ì◊®, ...      0            0  \n",
      "1  Let‚Äôs hope cooler heads prevail ‚Äî we‚Äôre runnin...      0            0  \n",
      "2  This is real-time history. Everyone should be ...      0            0  \n",
      "3  We may be watching the beginning of something ...      0            0  \n",
      "4  One strike leads to another. And another. This...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in FOX NEWS\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"NfiVPAQapw8\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_FOX_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31028e0e-2a7f-4b27-92e7-c04d8bcb153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 9541 and saved to israel_BBC_tGcVu2noaUU.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID              Author                 Time  \\\n",
      "0         C1  @WhetuMotutere-d3k  2025-07-17T16:20:00   \n",
      "1         C2  @WhetuMotutere-d3k  2025-07-17T16:19:47   \n",
      "2         C3  @WhetuMotutere-d3k  2025-07-17T16:19:20   \n",
      "3         C4  @WhetuMotutere-d3k  2025-07-17T16:18:33   \n",
      "4         C5       @EddieAyteniz  2025-07-14T09:36:57   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                                     Everyone knows      0            0  \n",
      "1                   America shouldn't be so stubborn      0            0  \n",
      "2                                        Its to late      0            0  \n",
      "3  Isreal flickers like a sparklee when building ...      0            0  \n",
      "4  LOL WHY YOU STOP WHAT HAPPENED METANYAHU U GOT...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in BBC 3\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"tGcVu2noaUU\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_BBC_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa2184e-e306-4fdc-99d7-6851670e3130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 2370 and saved to israel_10NEWS_5OBgBQVNFJM.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID             Author                 Time  \\\n",
      "0         C1    @Serandiptravel  2025-07-17T06:27:13   \n",
      "1         C2     @alihedari-p6o  2025-07-16T21:52:14   \n",
      "2         C3       @ryansyd4224  2025-07-15T22:55:37   \n",
      "3         C4          @ÿµÿßÿØŸÇ-ÿ∏1ŸÅ  2025-07-15T21:24:20   \n",
      "4         C5  @alimuntaziri4112  2025-07-15T18:58:04   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Golden cohen is not Melbourne tourist üòÇ he wen...      0            0  \n",
      "1  ÿßÿ≥ÿ±ÿßÿ¶€åŸÑ ÿ™Ÿàÿßÿ¨Ÿá ÿ≥Ÿàÿ±€åŸá Ÿà ÿ∫ÿ≤Ÿá ŸÑÿ®ŸÜÿßŸÜ €åŸÖŸÜ ÿ¢ŸÜÿ™Ÿá ÿßÿ®ŸÇŸá ...      0            0  \n",
      "2                            Bismillah Allahu Akbar!      0            0  \n",
      "3                                              üáÆüá∑üáÆüá∑ü´∞      0            0  \n",
      "4  Chosen peoe must tjink that Iramian missiles d...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in 10 NEWS\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"5OBgBQVNFJM\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_10NEWS_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff26a85a-29ff-4017-a0a0-ec19e720e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 1310 and saved to israel_ASSOCIATED_pYZ2sbB9d9o.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID                        Author                 Time  \\\n",
      "0         C1  @andreinikolaipagtananan6913  2025-07-17T06:33:04   \n",
      "1         C2                  @ÿ¨ŸÜ⁄Øÿ¨Ÿà_Ÿæÿßÿ±ÿ≥€å  2025-07-16T09:05:01   \n",
      "2         C3             @therazakshow1264  2025-07-14T15:47:47   \n",
      "3         C4   @iamalwaysrightandyouarenot  2025-07-13T14:58:52   \n",
      "4         C5                @zetepedje3414  2025-07-12T23:58:54   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0  Story\\nüáµüá∏: Iran! Iran!\\nüáÆüá∑:What Happened Pales...      0            0  \n",
      "1                                             ‚ù§Ô∏èüáÆüá∑üôèüèª      0            0  \n",
      "2  Fired Netanahu , Catch Netanahu and Love Jewis...      1            0  \n",
      "3                                               ü¶∏‚Äç‚ôÇÔ∏è      0            0  \n",
      "4  The US is desperately trying to maintain its i...      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in ASSOCIATE PRESS\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"pYZ2sbB9d9o\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_ASSOCIATED_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be0bf45-4c8a-4fc7-858a-b1cb8b7f8f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 3470 and saved to israel_BBC_Dqc2pkW4cX4.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID              Author                 Time  \\\n",
      "0         C1    @NgLianchuan-r2n  2025-07-05T03:35:35   \n",
      "1         C2     @MusaBojang-q3d  2025-07-03T14:10:55   \n",
      "2         C3     @giantearth5721  2025-06-26T11:48:58   \n",
      "3         C4  @MoisesCarhart-u1o  2025-06-22T09:13:56   \n",
      "4         C5         @Petsmania7  2025-06-22T08:30:00   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0      ‰∏çÊãøÊàò‰∫âÁàÜÂèëÂá∫Êù•Âì™Áßçüí∞ÔºÅËµöÈí±ÊúâÂæàÂ§öÁßçÊñπÂºèÔºÅÁúãÊàë‰ª¨ÂÅöËá™Â∑±ÁöÑÊ∞îÂÄôÂèòÂåñÊåáÂçóÂíåÁüøÁâ©Ë¥®‰∏çÊòØÊØîËæÉÂçéÁÆóÔºü      0            0  \n",
      "1  new slogan in West Israel have a right to exis...      0            0  \n",
      "2  Here is how to get saved and be right with God...      0            0  \n",
      "3  In using my first Amendment right in regards t...      0            0  \n",
      "4  Trump wants nobel prize but what he did is tot...      1            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in BBC 4\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"Dqc2pkW4cX4\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_BBC_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaf3567d-4dc1-49d5-8ddd-f722430c915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 556 and saved to israel_NBCNEWS__Y4cRHLwCNw.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID              Author                 Time  \\\n",
      "0         C1    @royalegamer9670  2025-07-17T02:48:27   \n",
      "1         C2         @tankerlori  2025-07-16T10:07:38   \n",
      "2         C3  @oscardipiazza3883  2025-07-15T05:11:01   \n",
      "3         C4        @maenadt9406  2025-07-15T04:53:41   \n",
      "4         C5  @saulalexander3586  2025-07-14T19:15:52   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0                      lovely to hear to see and see      0            0  \n",
      "1                                                üòÇüòÇüòÇ      0            0  \n",
      "2  HACERSE LAS VICTIMAS COMO SIEMPRE...JAJA..PERO...      0            0  \n",
      "3                                   Alhamdulillah..üéâ      0            0  \n",
      "4         Has not Trump  said  that the war is overüòÆ      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in NBCNEWS\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"_Y4cRHLwCNw\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_NBCNEWS_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "167f1759-a17e-4db4-8694-c790750a3dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 6143 and saved to israel_CGTN_7ppCP1jo7P4.xlsx\n",
      "\n",
      "First few rows of the output DataFrame:\n",
      "  Comment ID            Author                 Time  \\\n",
      "0         C1         @Taha-c7b  2025-07-17T17:57:08   \n",
      "1         C2  @AbolfazlSafinia  2025-07-17T16:49:13   \n",
      "2         C3       @ÿ™Ÿàÿ±ÿ¨ÿ™ŸÇ€åŸæŸàÿ±  2025-07-17T16:01:39   \n",
      "3         C4   @ManochehrAhanj  2025-07-17T04:36:59   \n",
      "4         C5   @iliyaafzal-i1x  2025-07-17T00:09:52   \n",
      "\n",
      "                                        Comment Text  Likes  Reply Count  \n",
      "0              ÿß€åŸÜ ŸáŸÖŸàŸÜ ⁄©ÿßŸÖŸÜÿ™ ŸÅÿßÿ±ÿ≥€å ⁄©Ÿá ÿØŸÜÿ®ÿßŸÑÿ¥ ŸÖ€å⁄Øÿ±ÿØ€å      1            0  \n",
      "1    ÿ™Ÿáÿ¥ ⁄©ÿ¥ÿ™ÿßÿ± ÿ®ÿ±ÿß€å ŸÖÿ±ÿØŸÖ ÿßÿ≥ÿ™ ⁄ÜŸá ÿ≥ŸàÿØ ŸàŸÇÿ™€å  ÿ™ŸÜŸáÿß Ÿáÿ≥ÿ™€åŸÖ      0            0  \n",
      "2  ŸÖŸàÿ¥⁄© Ÿáÿß€å ÿØŸáŸá €∑€∞ Ÿà€∏€∞ ÿ±Ÿà ŸÅÿ±ÿ≥ŸàÿØŸá ÿ®ŸàÿØŸÜ ÿØ€å⁄Ø ÿ¨ÿß€å€å Ÿàÿß...      0            0  \n",
      "3                              Israel is a big joke.      0            0  \n",
      "4                              Thank you. From Iran‚ù§      0            0  \n"
     ]
    }
   ],
   "source": [
    "# This code used to scrape comment fom a news video in CGTN\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \" \" # use your Youtube API ID in between here.\n",
    "\n",
    "# Replace with the ID of the YouTube video you want to scrape comments from\n",
    "video_id = \"7ppCP1jo7P4\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    \"\"\"Removes or replaces characters that can cause issues in Excel.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove characters that are not valid in XML\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def get_comments_for_excel(youtube, video_id, max_results=100):\n",
    "    \"\"\"\n",
    "    Retrieves comments and cleans the text for Excel compatibility.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_id_counter = 1\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_id = f\"C{comment_id_counter}\"\n",
    "                author = snippet['authorDisplayName']\n",
    "                timestamp = datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                time_formatted = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                comment_text = clean_text_for_excel(snippet['textDisplay'])  # Clean the text\n",
    "                likes = snippet['likeCount']\n",
    "                reply_count = item['snippet'].get('totalReplyCount', 0)\n",
    "\n",
    "                comments_data.append({\n",
    "                    'Comment ID': comment_id,\n",
    "                    'Author': author,\n",
    "                    'Time': time_formatted,\n",
    "                    'Comment Text': comment_text,\n",
    "                    'Likes': likes,\n",
    "                    'Reply Count': reply_count\n",
    "                })\n",
    "                comment_id_counter += 1\n",
    "\n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=max_results,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    textFormat=\"plainText\"\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Build the YouTube API service object\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the comments\n",
    "    comments = get_comments_for_excel(youtube, video_id, max_results=100)\n",
    "\n",
    "    if comments:\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(comments)\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_filename = f\"israel_CGTN_{video_id}.xlsx\"\n",
    "        try:\n",
    "            df.to_excel(excel_filename, index=False)\n",
    "            print(f\"Successfully scraped {len(comments)} and saved to {excel_filename}\")\n",
    "            print(\"\\nFirst few rows of the output DataFrame:\")\n",
    "            print(df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to Excel: {e}\")\n",
    "    else:\n",
    "        print(f\"No comments found for video ID: {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4ff1f-f4cd-40fa-88e7-2e9c35b83d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
